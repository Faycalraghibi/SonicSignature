{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Audio Fingerprinting & Bonus\n",
    "## AARES Project - Sonic Signature\n",
    "\n",
    "This notebook covers Exercises 4-5 and the Bonus:\n",
    "1.  **Audio Fingerprinting**: Implementing spectrograms, peak extraction, and hashing (Shazam algorithm).\n",
    "2.  **Database**: Building a search database from audio files.\n",
    "3.  **Conformal Prediction**: Quantifying uncertainty in genre classification.\n",
    "\n",
    "### Requirements\n",
    "- `librosa` for audio processing.\n",
    "- `numba` for optimized peak finding.\n",
    "- `utils_projet.py` (provided) for some helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import numba\n",
    "from numba import jit\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Add src/ or parent dir to path to find utils_projet if needed, \n",
    "# although we're copying code here or provided external file\n",
    "if os.path.exists('../utils_projet.py'):\n",
    "    sys.path.append('..')\n",
    "elif os.path.exists('utils_projet.py'):\n",
    "    sys.path.append('.')\n",
    "\n",
    "# Try importing utils_projet\n",
    "try:\n",
    "    from utils_projet import get_maxima, search_song, ConformalPrediction\n",
    "except ImportError:\n",
    "    print(\"Warning: utils_projet.py not found. Please ensure it is in the same directory or parent directory.\")\n",
    "    # Stubs if missing since we can't fully run without it\n",
    "    def get_maxima(x): return []\n",
    "    def search_song(db, hashes): return []\n",
    "    class ConformalPrediction: pass\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure plots\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_dataset_path(filename):\n",
    "    # Try current directory first\n",
    "    if os.path.exists(os.path.join('dataset', filename)):\n",
    "        return os.path.join('dataset', filename)\n",
    "    # Try parent directory (if notebook is in a subdirectory)\n",
    "    elif os.path.exists(os.path.join('..', 'dataset', filename)):\n",
    "        return os.path.join('..', 'dataset', filename)\n",
    "    else:\n",
    "        # Fallback/Error\n",
    "        raise FileNotFoundError(f\"Could not find {{filename}} in dataset/ or ../dataset/\")\n",
    "\n",
    "def get_songs_dir():\n",
    "    if os.path.isdir('songs'):\n",
    "        return 'songs'\n",
    "    elif os.path.isdir('../songs'):\n",
    "        return '../songs'\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find songs/ directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Fingerprinting Implementation\n",
    "\n",
    "We implement the core components of the audio fingerprinting system:\n",
    "- `compute_spectrogram`: STFT magnitude squared.\n",
    "- `get_maxima_in_tz`: Finding local peaks in a Target Zone (TZ).\n",
    "- `get_hashes`: Generating pairs and hashes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fingerprinting Core (adapted from src/fingerprinting.py)\n",
    "\n",
    "def compute_spectrogram(x, sr=3000):\n",
    "    '''Computes the energy spectrogram.'''\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    win_length = 1024\n",
    "    D = librosa.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    S = np.abs(D)**2\n",
    "    return S\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_maxima_in_tz(S, maxima, anchor):\n",
    "    '''Finds top maxima in the Target Zone (300 time x 20 freq) around an anchor.'''\n",
    "    f_anchor, t_anchor = anchor[0], anchor[1]\n",
    "    \n",
    "    t_min = t_anchor + 10 \n",
    "    t_max = t_anchor + 300\n",
    "    f_min = max(0, f_anchor - 10)\n",
    "    f_max = min(S.shape[0], f_anchor + 10)\n",
    "    \n",
    "    # 1. Count candidates\n",
    "    count = 0\n",
    "    for i in range(len(maxima)):\n",
    "        f = maxima[i][0]\n",
    "        t = maxima[i][1]\n",
    "        if (t >= t_min) and (t <= t_max) and (f >= f_min) and (f <= f_max):\n",
    "            count += 1\n",
    "            \n",
    "    if count == 0:\n",
    "        return np.empty((0, 2), dtype=np.int64)\n",
    "        \n",
    "    # 2. Fill candidates\n",
    "    candidate_indices = np.empty(count, dtype=np.int64)\n",
    "    current_idx = 0\n",
    "    for i in range(len(maxima)):\n",
    "        f = maxima[i][0]\n",
    "        t = maxima[i][1]\n",
    "        if (t >= t_min) and (t <= t_max) and (f >= f_min) and (f <= f_max):\n",
    "            candidate_indices[current_idx] = i\n",
    "            current_idx += 1\n",
    "            \n",
    "    # 3. Get amplitudes\n",
    "    candidate_amps = np.empty(count, dtype=np.float64)\n",
    "    candidate_coords = np.empty((count, 2), dtype=np.int64)\n",
    "    for k in range(count):\n",
    "        idx = candidate_indices[k]\n",
    "        f = int(maxima[idx][0])\n",
    "        t = int(maxima[idx][1])\n",
    "        candidate_amps[k] = S[f, t]\n",
    "        candidate_coords[k, 0] = f\n",
    "        candidate_coords[k, 1] = t\n",
    "        \n",
    "    # 4. Sort and take top 10\n",
    "    order = np.argsort(candidate_amps)[::-1]\n",
    "    n_top = min(10, count)\n",
    "    top_indices = order[:n_top]\n",
    "    \n",
    "    result = np.empty((n_top, 2), dtype=np.int64)\n",
    "    for k in range(n_top):\n",
    "        result[k] = candidate_coords[top_indices[k]]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_hashes(anchor, maxima_in_tz):\n",
    "    '''Generates hashes: anchor_freq * 1M + point_freq * 1K + delta_time.'''\n",
    "    hashes = []\n",
    "    f_anchor, t_anchor = int(anchor[0]), int(anchor[1])\n",
    "    \n",
    "    for point in maxima_in_tz:\n",
    "        f_point, t_point = int(point[0]), int(point[1])\n",
    "        delta_t = t_point - t_anchor\n",
    "        hash_val = f_anchor * 1000000 + f_point * 1000 + delta_t\n",
    "        hashes.append((hash_val, t_anchor))\n",
    "        \n",
    "    return hashes\n",
    "\n",
    "def process_signal(x, sr=3000):\n",
    "    '''Pipeline: Signal -> Spectrogram -> Maxima -> Hashes'''\n",
    "    S = compute_spectrogram(x, sr=sr)\n",
    "    maxima = get_maxima(S) # From utils_projet\n",
    "    \n",
    "    hashes_dict = {}\n",
    "    for i in range(len(maxima)):\n",
    "        anchor = maxima[i]\n",
    "        maxima_in_tz = get_maxima_in_tz(S, maxima, anchor)\n",
    "        new_hashes = get_hashes(anchor, maxima_in_tz)\n",
    "        for h, t in new_hashes:\n",
    "            hashes_dict[h] = t\n",
    "            \n",
    "    return hashes_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Database and Search\n",
    "\n",
    "We build a database of fingerprints from the `songs/` directory and implement search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13 songs.\n",
      "Searching for ../songs\\Classicals.de - Bizet, Georges - Carmen Prelude - Arranged for Solo Piano.mp3...\n",
      "Top Matches:\n",
      "- Classicals.de - Bizet, Georges - Carmen Prelude - Arranged for Solo Piano.mp3\n",
      "- Classicals.de - Debussy - Suite Bergamasque - 2. Menuet - L.75.mp3\n",
      "- Classicals.de - Debussy - Suite Bergamasque - 4. Passepied - L.75.mp3\n"
     ]
    }
   ],
   "source": [
    "# Database Functions (adapted from src/database.py)\n",
    "\n",
    "class AudioDatabase:\n",
    "    def __init__(self, db_path='dataset/dataset.pickle'):\n",
    "        # Fix db_path based on location\n",
    "        if not os.path.exists('dataset') and os.path.exists('../dataset'):\n",
    "             self.db_path = '../dataset/dataset.pickle'\n",
    "        else:\n",
    "             self.db_path = db_path\n",
    "             \n",
    "        self.database = []\n",
    "        self.song_names = []\n",
    "    \n",
    "    def create_database(self, songs_dir_name='songs'):\n",
    "        songs_dir = get_songs_dir()\n",
    "        print(f\"Creating database from {songs_dir}...\")\n",
    "        \n",
    "        audio_extensions = ['*.mp3', '*.wav', '*.flac']\n",
    "        audio_files = []\n",
    "        for ext in audio_extensions:\n",
    "            audio_files.extend(glob.glob(os.path.join(songs_dir, ext)))\n",
    "            \n",
    "        print(f\"Found {len(audio_files)} files.\")\n",
    "        \n",
    "        for file_path in audio_files:\n",
    "            try:\n",
    "                print(f\"Processing {file_path}...\")\n",
    "                y, sr = librosa.load(file_path, sr=3000)\n",
    "                hashes = process_signal(y, sr=sr)\n",
    "                self.database.append(hashes)\n",
    "                self.song_names.append(os.path.basename(file_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                \n",
    "        # Ensure dir exists for db_path\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, 'wb') as handle:\n",
    "            pickle.dump({'hashes': self.database, 'names': self.song_names}, handle)\n",
    "        print(\"Database saved.\")\n",
    "        \n",
    "    def load_database(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            print(f\"Database not found at {self.db_path}.\")\n",
    "            return False\n",
    "        with open(self.db_path, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            self.database = data['hashes']\n",
    "            self.song_names = data['names']\n",
    "        print(f\"Loaded {len(self.database)} songs.\")\n",
    "        return True\n",
    "\n",
    "    def search_song(self, excerpt_path):\n",
    "        if not self.database: # Load if empty\n",
    "            if not self.load_database(): return\n",
    "            \n",
    "        print(f\"Searching for {excerpt_path}...\")\n",
    "        try:\n",
    "            y, sr = librosa.load(excerpt_path, sr=3000)\n",
    "            song_hashes = process_signal(y, sr=sr)\n",
    "            \n",
    "            top_indices = search_song(self.database, song_hashes)\n",
    "            \n",
    "            print(\"Top Matches:\")\n",
    "            for i in top_indices:\n",
    "                if i < len(self.song_names):\n",
    "                    print(f\"- {self.song_names[i]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "\n",
    "# Demo\n",
    "try:\n",
    "    db = AudioDatabase()\n",
    "    # To rebuild: db.create_database()\n",
    "    if db.load_database():\n",
    "        if len(db.song_names) > 0:\n",
    "            songs_dir = get_songs_dir()\n",
    "            test_song = os.path.join(songs_dir, db.song_names[0])\n",
    "            db.search_song(test_song)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Conformal Prediction\n",
    "\n",
    "We use Conformal Prediction to produce valid prediction intervals with a guaranteed error rate (epsilon).\n",
    "We use a KNN classifier on the first 1500 samples of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Conformal Prediction ---\n",
      "Training KNN...\n",
      "blues\n",
      "chanson\n",
      "classical\n",
      "country\n",
      "dance\n",
      "disco\n",
      "edm\n",
      "electro\n",
      "folk\n",
      "hip hop\n",
      "jazz\n",
      "latin\n",
      "metal\n",
      "pop\n",
      "punk\n",
      "r&b\n",
      "rap\n",
      "reggae\n",
      "rock\n",
      "salsa\n",
      "soul\n",
      "techno\n",
      "\n",
      "Intervals for test point:\n",
      "Eps: 0.05 (Confidence: 95%) -> ['blues', 'classical', 'country', 'dance', 'disco', 'edm', 'electro', 'folk', 'hip hop', 'jazz', 'latin', 'pop', 'r&b', 'rap', 'rock', 'soul']\n",
      "Eps: 0.1 (Confidence: 90%) -> ['classical', 'country', 'dance', 'edm', 'electro', 'folk', 'hip hop', 'jazz', 'pop', 'r&b', 'rap', 'soul']\n",
      "Eps: 0.2 (Confidence: 80%) -> ['country', 'electro', 'folk', 'hip hop', 'pop', 'r&b', 'rap', 'soul']\n"
     ]
    }
   ],
   "source": [
    "# Conformal Prediction Analysis (adapted from src/conformal.py)\n",
    "\n",
    "def run_conformal_analysis():\n",
    "    print(\"--- Conformal Prediction ---\")\n",
    "    \n",
    "    try:\n",
    "        train_path = get_dataset_path('spotify_dataset_train.csv')\n",
    "        test_path = get_dataset_path('spotify_dataset_test.csv')\n",
    "        \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        # Extract year if missing\n",
    "        for df in [train_df, test_df]:\n",
    "            if 'year' not in df.columns and 'release_date' in df.columns:\n",
    "                 df['year'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year\n",
    "        \n",
    "        # Subset\n",
    "        train_subset = train_df.iloc[:1500]\n",
    "        \n",
    "        feature_cols = ['acousticness', 'danceability', 'energy', 'duration_ms', \n",
    "                        'instrumentalness', 'valence', 'popularity', 'tempo', \n",
    "                        'liveness', 'loudness', 'speechiness', 'year']\n",
    "        \n",
    "        X_train = train_subset[feature_cols].fillna(0).values\n",
    "        y_train = train_subset['genre'].values\n",
    "        \n",
    "        X_test = test_df[feature_cols].fillna(0).values\n",
    "        x_new = X_test[0] # Test point\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        x_new_scaled = scaler.transform(x_new.reshape(1, -1))[0]\n",
    "        \n",
    "        # Train KNN\n",
    "        print(\"Training KNN...\")\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Conformal\n",
    "        predictor = ConformalPrediction(knn, X_train_scaled, y_train)\n",
    "        predictor.predict(x_new_scaled)\n",
    "        \n",
    "        # Intervals\n",
    "        print(\"\\nIntervals for test point:\")\n",
    "        for eps in [0.05, 0.1, 0.2]:\n",
    "            interval = predictor.compute_interval(eps)\n",
    "            print(f\"Eps: {eps} (Confidence: {1-eps:.0%}) -> {interval}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "run_conformal_analysis()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
